-------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/sokbaelee/Dropbox/Persuasion/JPE/Replication Files/scripts/logs/2022.11.11-08.09.16.log.txt
  log type:  text
 opened on:  11 Nov 2022, 08:09:16

. 
. di "Begin date and time: $S_DATE $S_TIME"
Begin date and time: 11 Nov 2022 08:09:16

. di "Stata version: `c(stata_version)'"
Stata version: 17

. di "Updated as of: `c(born_date)'"
Updated as of: 23 Aug 2022

. di "Variant:       `=cond( c(MP),"MP",cond(c(SE),"SE",c(flavor)) )'"
Variant:       MP

. di "Processors:    `c(processors)'"
Processors:    4

. di "OS:            `c(os)' `c(osdtl)'"
OS:            MacOSX 12.5.1

. di "Machine type:  `c(machine_type)'"
Machine type:  Mac (Apple Silicon)

. 
. * Stata version control
. version 16

. 
. * Create directories for output files
. cap mkdir "$Persuasion/results"

. 
. **********************
. * Run analysis
. **********************
. * TABLE 1. Persuasion Rates: Papers on Voter Turnout
. *do "$Persuasion/scripts/table1.do"
. 
. * Effects of Uncensored Media: Revisiting Chen and Yang (2019)
. *   To generate the dataset "ChenYang2019.dta", it is necessary to run CY19_data.do 
. *   after downloading the original dataset from 
. *   the AER webpage at https://www.aeaweb.org/articles?id=10.1257/aer.20171765
. *   and storing them at "$Persuasion/data/ChenYang2019".
. *   As the dataset is already created and stored at "$Persuasion/data/ChenYang2019", 
. *   the do file is commented below.
. *do "$Persuasion/scripts/CY19_data.do"
. *   TABLE 2. Persuasion Rates of Exposure to Uncensored Internet
. *do "$Persuasion/scripts/table2.do"
. 
. * Effects of Political News: Revisting Gerber, Karlan, and Bergan (2009) 
. *   TABLE 3. Summary Statistics of the GKB Data
. *do "$Persuasion/scripts/table3.do"
. *   TABLE 4. Estimates of the Key Parameters
. *do "$Persuasion/scripts/table4.do"
. 
. *       TABLE D1. Persuasion Rates: Fox News Effects
. *do "$Persuasion/scripts/tableD1.do"
. 
. *       FIGURE D1. Estimates of Marginal and Average Persuasion Rates
. do "$Persuasion/scripts/figureD1.do"

. ************
. * SCRIPT:   figureD1.do
. * PURPOSE:  Creates Figure D1
. *
. * ACKNOWLEDGMENT
. *       The orginal dataset "NTV_Individual_Data.dta" is from Enikolopov, Petrova, and Zhuravskaya (AER, 2011).
. ************
. 
. use "$Persuasion/data/EnikolopovPetrovaZhuravskaya2011/NTV_Individual_Data.dta", clear

. 
. local basic = "logpop98 wage98_ln"

. local sociodem = "male age educ1 married consump"

. 
. local party_list = "Unity OVR"

. 
. *** create outcome variables based on directional treatment ***
. 
. gen y_vote_Unity = 0

. replace y_vote_Unity = 1 if vote_Unity == 0   
(993 real changes made)

. 
. gen y_vote_OVR = 0

. replace y_vote_OVR = 1 if vote_OVR == 1   
(166 real changes made)

. 
. *** predict the exposure rate, that is, e(X,Z) = P(T=1|X,Z) ***
. *** specification: linear in Z and X ***
. 
. probit  Watches_NTV_1999 tvmaxtveloss5050powerA `sociodem' `basic' [pweight=kishweig]

Iteration 0:   log pseudolikelihood = -809.06705  
Iteration 1:   log pseudolikelihood = -741.86673  
Iteration 2:   log pseudolikelihood =  -741.6003  
Iteration 3:   log pseudolikelihood = -741.60028  

Probit regression                                       Number of obs =  1,185
                                                        Wald chi2(8)  = 105.94
                                                        Prob > chi2   = 0.0000
Log pseudolikelihood = -741.60028                       Pseudo R2     = 0.0834

----------------------------------------------------------------------------------------
                       |               Robust
      Watches_NTV_1999 | Coefficient  std. err.      z    P>|z|     [95% conf. interval]
-----------------------+----------------------------------------------------------------
tvmaxtveloss5050powerA |   .0068679   .0011649     5.90   0.000     .0045848    .0091511
                  male |   .2299747   .0893844     2.57   0.010     .0547845    .4051648
                   age |  -.0063456   .0029081    -2.18   0.029    -.0120454   -.0006458
                 educ1 |   .2459444   .1188836     2.07   0.039     .0129369    .4789519
               married |   .0520022   .0908934     0.57   0.567    -.1261455    .2301499
               consump |   .0711412   .0341425     2.08   0.037     .0042231    .1380594
              logpop98 |  -.2162026    .045017    -4.80   0.000    -.3044343    -.127971
             wage98_ln |   .4838216   .1418335     3.41   0.001     .2058331    .7618101
                 _cons |  -2.154867   .9164043    -2.35   0.019    -3.950986   -.3587474
----------------------------------------------------------------------------------------

. predict phat
(option pr assumed; Pr(Watches_NTV_1999))
(4,658 missing values generated)

. 
. ***
. 
. foreach party in `party_list' {
  2. 
.         * generate Y*(1-T)
.     gen notwatch_vote_`party' = 0
  3.         replace notwatch_vote_`party' = 1 if y_vote_`party' == 1 & Watches_NTV_1999==0
  4.         
.         * linear probability model of Y given X and e(X,Z)
.         * specification: linear in X and cubic in e(X,Z), while interacting some of X and e(X,Z)
.         reg y_vote_`party' `sociodem' `basic' ///
>                         c.phat c.phat#c.phat c.phat#c.phat#c.phat ///
>                         c.phat#i.male c.phat#c.age c.phat#i.educ1 c.phat#i.married c.phat#c.consump ///
>                         [pweight=kishweig]
  5.         
.         margins [pweight=kishweig], dydx(phat) at(phat = (0.01(0.01)0.99))
  6. 
. }
(313 real changes made)
(sum of wgt is 1,312.04910933971)

Linear regression                               Number of obs     =      1,300
                                                F(15, 1284)       =       4.23
                                                Prob > F          =     0.0000
                                                R-squared         =     0.0511
                                                Root MSE          =      .4882

--------------------------------------------------------------------------------------
                     |               Robust
        y_vote_Unity | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
---------------------+----------------------------------------------------------------
                male |   .1476635   .1403743     1.05   0.293    -.1277246    .4230516
                 age |   .0097242   .0041921     2.32   0.021     .0015001    .0179483
               educ1 |   .0741608      .1628     0.46   0.649    -.2452224     .393544
             married |   .0404414   .1280636     0.32   0.752    -.2107953    .2916782
             consump |  -.0573286   .0551864    -1.04   0.299     -.165594    .0509367
            logpop98 |   .0070906   .0160963     0.44   0.660    -.0244874    .0386686
           wage98_ln |  -.1845544   .0687819    -2.68   0.007    -.3194917   -.0496171
                phat |   .9654167   2.375671     0.41   0.685    -3.695206    5.626039
                     |
       c.phat#c.phat |  -1.435164   4.236805    -0.34   0.735    -9.746985    6.876657
                     |
c.phat#c.phat#c.phat |   1.285474   2.421765     0.53   0.596    -3.465577    6.036524
                     |
         male#c.phat |
                  1  |   -.282562   .2180701    -1.30   0.195    -.7103749    .1452509
                     |
        c.phat#c.age |  -.0040113   .0068059    -0.59   0.556    -.0173633    .0093406
                     |
        educ1#c.phat |
                  1  |  -.0275575   .2982328    -0.09   0.926    -.6126345    .5575195
                     |
      married#c.phat |
                  1  |  -.0101435   .2089699    -0.05   0.961    -.4201034    .3998165
                     |
    c.phat#c.consump |   .0738618   .0804907     0.92   0.359    -.0840459    .2317696
                     |
               _cons |   1.129342   .5571958     2.03   0.043     .0362275    2.222456
--------------------------------------------------------------------------------------

Average marginal effects                                 Number of obs = 1,300
Model VCE: Robust

Expression: Linear prediction, predict()
dy/dx wrt:  phat
1._at:  phat = .01
2._at:  phat = .02
3._at:  phat = .03
4._at:  phat = .04
5._at:  phat = .05
6._at:  phat = .06
7._at:  phat = .07
8._at:  phat = .08
9._at:  phat = .09
10._at: phat =  .1
11._at: phat = .11
12._at: phat = .12
13._at: phat = .13
14._at: phat = .14
15._at: phat = .15
16._at: phat = .16
17._at: phat = .17
18._at: phat = .18
19._at: phat = .19
20._at: phat =  .2
21._at: phat = .21
22._at: phat = .22
23._at: phat = .23
24._at: phat = .24
25._at: phat = .25
26._at: phat = .26
27._at: phat = .27
28._at: phat = .28
29._at: phat = .29
30._at: phat =  .3
31._at: phat = .31
32._at: phat = .32
33._at: phat = .33
34._at: phat = .34
35._at: phat = .35
36._at: phat = .36
37._at: phat = .37
38._at: phat = .38
39._at: phat = .39
40._at: phat =  .4
41._at: phat = .41
42._at: phat = .42
43._at: phat = .43
44._at: phat = .44
45._at: phat = .45
46._at: phat = .46
47._at: phat = .47
48._at: phat = .48
49._at: phat = .49
50._at: phat =  .5
51._at: phat = .51
52._at: phat = .52
53._at: phat = .53
54._at: phat = .54
55._at: phat = .55
56._at: phat = .56
57._at: phat = .57
58._at: phat = .58
59._at: phat = .59
60._at: phat =  .6
61._at: phat = .61
62._at: phat = .62
63._at: phat = .63
64._at: phat = .64
65._at: phat = .65
66._at: phat = .66
67._at: phat = .67
68._at: phat = .68
69._at: phat = .69
70._at: phat =  .7
71._at: phat = .71
72._at: phat = .72
73._at: phat = .73
74._at: phat = .74
75._at: phat = .75
76._at: phat = .76
77._at: phat = .77
78._at: phat = .78
79._at: phat = .79
80._at: phat =  .8
81._at: phat = .81
82._at: phat = .82
83._at: phat = .83
84._at: phat = .84
85._at: phat = .85
86._at: phat = .86
87._at: phat = .87
88._at: phat = .88
89._at: phat = .89
90._at: phat =  .9
91._at: phat = .91
92._at: phat = .92
93._at: phat = .93
94._at: phat = .94
95._at: phat = .95
96._at: phat = .96
97._at: phat = .97
98._at: phat = .98
99._at: phat = .99

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
phat         |
         _at |
          1  |   .7951023   2.336328     0.34   0.734    -3.788337    5.378542
          2  |    .767556   2.255319     0.34   0.734    -3.656959    5.192071
          3  |   .7407809   2.175779     0.34   0.734    -3.527692    5.009254
          4  |   .7147771    2.09771     0.34   0.733    -3.400538    4.830092
          5  |   .6895446   2.021112     0.34   0.733      -3.2755     4.65459
          6  |   .6650834   1.945988     0.34   0.733    -3.152582    4.482748
          7  |   .6413935   1.872338     0.34   0.732    -3.031785    4.314572
          8  |   .6184748   1.800165     0.34   0.731    -2.913114    4.150063
          9  |   .5963274   1.729471     0.34   0.730    -2.796572    3.989227
         10  |   .5749514   1.660258     0.35   0.729    -2.682164    3.832067
         11  |   .5543466   1.592527     0.35   0.728    -2.569895    3.678588
         12  |   .5345131   1.526283     0.35   0.726    -2.459769    3.528795
         13  |   .5154508   1.461528     0.35   0.724    -2.351793    3.382695
         14  |   .4971599   1.398264     0.36   0.722    -2.245974    3.240293
         15  |   .4796402   1.336497     0.36   0.720    -2.142317    3.101598
         16  |   .4628918    1.27623     0.36   0.717    -2.040832    2.966616
         17  |   .4469147   1.217467     0.37   0.714    -1.941528    2.835357
         18  |   .4317089   1.160214     0.37   0.710    -1.844413    2.707831
         19  |   .4172744   1.104476     0.38   0.706    -1.749501     2.58405
         20  |   .4036112    1.05026     0.38   0.701    -1.656803    2.464025
         21  |   .3907192   .9975733     0.39   0.695    -1.566333    2.347772
         22  |   .3785985   .9464239     0.40   0.689    -1.478109    2.235306
         23  |   .3672491   .8968211     0.41   0.682    -1.392146    2.126645
         24  |    .356671   .8487754     0.42   0.674    -1.308468     2.02181
         25  |   .3468642   .8022984     0.43   0.666    -1.227095    1.920824
         26  |   .3378287   .7574037     0.45   0.656    -1.148056    1.823713
         27  |   .3295644   .7141062     0.46   0.645    -1.071379    1.730508
         28  |   .3220715   .6724234     0.48   0.632    -.9970977    1.641241
         29  |   .3153498   .6323745     0.50   0.618    -.9252509     1.55595
         30  |   .3093994   .5939813     0.52   0.603    -.8558811     1.47468
         31  |   .3042203   .5572689     0.55   0.585    -.7890373    1.397478
         32  |   .2998124   .5222648     0.57   0.566    -.7247737    1.324399
         33  |   .2961759   .4889999     0.61   0.545    -.6631507    1.255502
         34  |   .2933106   .4575086     0.64   0.522    -.6042358    1.190857
         35  |   .2912166   .4278279     0.68   0.496    -.5481019    1.130535
         36  |    .289894   .3999978     0.72   0.469     -.494827    1.074615
         37  |   .2893425   .3740599     0.77   0.439    -.4444932    1.023178
         38  |   .2895624   .3500561     0.83   0.408    -.3971823    .9763072
         39  |   .2905536    .328026     0.89   0.376    -.3529721    .9340793
         40  |    .292316   .3080036     0.95   0.343    -.3119294    .8965615
         41  |   .2948497   .2900131     1.02   0.309    -.2741018    .8638013
         42  |   .2981548   .2740634     1.09   0.277    -.2395064    .8358159
         43  |   .3022311   .2601416     1.16   0.246    -.2081182    .8125803
         44  |   .3070786    .248208     1.24   0.216    -.1798592    .7940165
         45  |   .3126975   .2381909     1.31   0.189    -.1545886    .7799836
         46  |   .3190876   .2299844     1.39   0.166    -.1320988    .7702741
         47  |   .3262491   .2234498     1.46   0.145    -.1121177    .7646159
         48  |   .3341818   .2184201     1.53   0.126    -.0943176    .7626812
         49  |   .3428858   .2147079     1.60   0.111    -.0783311    .7641027
         50  |   .3523611   .2121159     1.66   0.097    -.0637707    .7684929
         51  |   .3626076   .2104466     1.72   0.085    -.0502494    .7754646
         52  |   .3736255   .2095126     1.78   0.075    -.0373991      .78465
         53  |   .3854146   .2091439     1.84   0.066    -.0248867    .7957159
         54  |   .3979751   .2091943     1.90   0.057    -.0124251    .8083753
         55  |   .4113068   .2095445     1.96   0.050     .0002196     .822394
         56  |   .4254098   .2101046     2.02   0.043     .0132238    .8375957
         57  |    .440284   .2108147     2.09   0.037     .0267049    .8538632
         58  |   .4559296   .2116455     2.15   0.031     .0407207    .8711384
         59  |   .4723464   .2125973     2.22   0.026     .0552702    .8894226
         60  |   .4895346   .2137002     2.29   0.022     .0702946    .9087745
         61  |    .507494   .2150128     2.36   0.018     .0856791    .9293089
         62  |   .5262247   .2166207     2.43   0.015     .1012553     .951194
         63  |   .5457266   .2186354     2.50   0.013     .1168049    .9746484
         64  |   .5659999   .2211907     2.56   0.011     .1320649    .9999348
         65  |   .5870444   .2244396     2.62   0.009     .1467359    1.027353
         66  |   .6088603   .2285482     2.66   0.008     .1604915    1.057229
         67  |   .6314474   .2336898     2.70   0.007     .1729916    1.089903
         68  |   .6548058   .2400375     2.73   0.006     .1838971    1.125714
         69  |   .6789355   .2477557     2.74   0.006      .192885    1.164986
         70  |   .7038364    .256994     2.74   0.006     .1996623    1.208011
         71  |   .7295086   .2678802     2.72   0.007     .2039778     1.25504
         72  |   .7559523   .2805176     2.69   0.007     .2056291    1.306276
         73  |   .7831671   .2949832     2.65   0.008     .2044652    1.361869
         74  |   .8111532   .3113285     2.61   0.009     .2003848    1.421922
         75  |   .8399106   .3295828     2.55   0.011     .1933307     1.48649
         76  |   .8694392   .3497561     2.49   0.013     .1832831    1.555595
         77  |   .8997391   .3718438     2.42   0.016      .170251    1.629227
         78  |   .9308103   .3958306     2.35   0.019     .1542646    1.707356
         79  |    .962653    .421694     2.28   0.023     .1353682    1.789938
         80  |   .9952668   .4494065     2.21   0.027     .1136152    1.876918
         81  |   1.028652   .4789391     2.15   0.032     .0890627    1.968241
         82  |   1.062808   .5102622     2.08   0.037     .0617691    2.063847
         83  |   1.097736   .5433466     2.02   0.044     .0317913     2.16368
         84  |   1.133435   .5781645     1.96   0.050     -.000816    2.267686
         85  |   1.169905   .6146902     1.90   0.057    -.0360023    2.375813
         86  |   1.207147   .6528993     1.85   0.065    -.0737199    2.488013
         87  |   1.245159     .69277     1.80   0.073    -.1139259    2.604245
         88  |   1.283943   .7342823     1.75   0.081    -.1565813    2.724468
         89  |   1.323499   .7774183     1.70   0.089    -.2016507    2.848648
         90  |   1.363825   .8221617     1.66   0.097    -.2491023    2.976753
         91  |   1.404924   .8684982     1.62   0.106    -.2989077    3.108755
         92  |   1.446793   .9164141     1.58   0.115    -.3510407    3.244626
         93  |   1.489433   .9658982     1.54   0.123    -.4054787    3.384345
         94  |   1.532845    1.01694     1.51   0.132     -.462201    3.527891
         95  |   1.577028    1.06953     1.47   0.141    -.5211892    3.675245
         96  |   1.621982   1.123659     1.44   0.149    -.5824268    3.826391
         97  |   1.667708   1.179321     1.41   0.158    -.6458993    3.981316
         98  |   1.714205   1.236508     1.39   0.166    -.7115926    4.140003
         99  |   1.761473   1.295214     1.36   0.174    -.7794951    4.302442
------------------------------------------------------------------------------
(30 real changes made)
(sum of wgt is 1,312.04910933971)

Linear regression                               Number of obs     =      1,300
                                                F(15, 1284)       =       2.22
                                                Prob > F          =     0.0045
                                                R-squared         =     0.0306
                                                Root MSE          =     .24958

--------------------------------------------------------------------------------------
                     |               Robust
          y_vote_OVR | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
---------------------+----------------------------------------------------------------
                male |     .02376   .0647407     0.37   0.714    -.1032492    .1507691
                 age |  -.0067533   .0030283    -2.23   0.026    -.0126943   -.0008123
               educ1 |   .0304907   .0970989     0.31   0.754    -.1599993    .2209807
             married |   .0013934   .0633744     0.02   0.982    -.1229353    .1257222
             consump |   .0451769   .0307046     1.47   0.141    -.0150598    .1054137
            logpop98 |    .004429   .0089747     0.49   0.622    -.0131777    .0220356
           wage98_ln |  -.0712375   .0392169    -1.82   0.070    -.1481738    .0056989
                phat |  -1.736624   1.194254    -1.45   0.146    -4.079527    .6062791
                     |
       c.phat#c.phat |   2.337432   2.215498     1.06   0.292    -2.008962    6.683826
                     |
c.phat#c.phat#c.phat |  -.8379979   1.294881    -0.65   0.518    -3.378312    1.702316
                     |
         male#c.phat |
                  1  |  -.1201737   .1020264    -1.18   0.239    -.3203304    .0799829
                     |
        c.phat#c.age |   .0133482    .004851     2.75   0.006     .0038313     .022865
                     |
        educ1#c.phat |
                  1  |  -.1100876   .1817245    -0.61   0.545    -.4665972    .2464219
                     |
      married#c.phat |
                  1  |   -.010471   .1013051    -0.10   0.918    -.2092128    .1882707
                     |
    c.phat#c.consump |  -.0740195   .0430061    -1.72   0.085    -.1583895    .0103505
                     |
               _cons |   .9225595   .2895067     3.19   0.001     .3546013    1.490518
--------------------------------------------------------------------------------------

Average marginal effects                                 Number of obs = 1,300
Model VCE: Robust

Expression: Linear prediction, predict()
dy/dx wrt:  phat
1._at:  phat = .01
2._at:  phat = .02
3._at:  phat = .03
4._at:  phat = .04
5._at:  phat = .05
6._at:  phat = .06
7._at:  phat = .07
8._at:  phat = .08
9._at:  phat = .09
10._at: phat =  .1
11._at: phat = .11
12._at: phat = .12
13._at: phat = .13
14._at: phat = .14
15._at: phat = .15
16._at: phat = .16
17._at: phat = .17
18._at: phat = .18
19._at: phat = .19
20._at: phat =  .2
21._at: phat = .21
22._at: phat = .22
23._at: phat = .23
24._at: phat = .24
25._at: phat = .25
26._at: phat = .26
27._at: phat = .27
28._at: phat = .28
29._at: phat = .29
30._at: phat =  .3
31._at: phat = .31
32._at: phat = .32
33._at: phat = .33
34._at: phat = .34
35._at: phat = .35
36._at: phat = .36
37._at: phat = .37
38._at: phat = .38
39._at: phat = .39
40._at: phat =  .4
41._at: phat = .41
42._at: phat = .42
43._at: phat = .43
44._at: phat = .44
45._at: phat = .45
46._at: phat = .46
47._at: phat = .47
48._at: phat = .48
49._at: phat = .49
50._at: phat =  .5
51._at: phat = .51
52._at: phat = .52
53._at: phat = .53
54._at: phat = .54
55._at: phat = .55
56._at: phat = .56
57._at: phat = .57
58._at: phat = .58
59._at: phat = .59
60._at: phat =  .6
61._at: phat = .61
62._at: phat = .62
63._at: phat = .63
64._at: phat = .64
65._at: phat = .65
66._at: phat = .66
67._at: phat = .67
68._at: phat = .68
69._at: phat = .69
70._at: phat =  .7
71._at: phat = .71
72._at: phat = .72
73._at: phat = .73
74._at: phat = .74
75._at: phat = .75
76._at: phat = .76
77._at: phat = .77
78._at: phat = .78
79._at: phat = .79
80._at: phat =  .8
81._at: phat = .81
82._at: phat = .82
83._at: phat = .83
84._at: phat = .84
85._at: phat = .85
86._at: phat = .86
87._at: phat = .87
88._at: phat = .88
89._at: phat = .89
90._at: phat =  .9
91._at: phat = .91
92._at: phat = .92
93._at: phat = .93
94._at: phat = .94
95._at: phat = .95
96._at: phat = .96
97._at: phat = .97
98._at: phat = .98
99._at: phat = .99

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
phat         |
         _at |
          1  |  -1.549934   1.170622    -1.32   0.186    -3.846475     .746608
          2  |  -1.503939   1.128412    -1.33   0.183    -3.717674    .7097951
          3  |  -1.458448   1.086996    -1.34   0.180    -3.590931    .6740363
          4  |  -1.413459   1.046375    -1.35   0.177    -3.466252     .639334
          5  |  -1.368973    1.00655    -1.36   0.174    -3.343636    .6056909
          6  |  -1.324989    .967523    -1.37   0.171    -3.223089    .5731099
          7  |  -1.281509   .9292951    -1.38   0.168    -3.104612    .5415943
          8  |  -1.238531   .8918684    -1.39   0.165    -2.988211    .5111478
          9  |  -1.196057   .8552448    -1.40   0.162    -2.873887    .4817742
         10  |  -1.154084   .8194268    -1.41   0.159    -2.761647    .4534779
         11  |  -1.112615   .7844167    -1.42   0.156    -2.651494    .4262638
         12  |  -1.071649   .7502172    -1.43   0.153    -2.543435    .4001373
         13  |  -1.031185   .7168314    -1.44   0.151    -2.437474    .3751042
         14  |  -.9912243   .6842627    -1.45   0.148     -2.33362    .3511713
         15  |  -.9517662   .6525148    -1.46   0.145    -2.231878     .328346
         16  |   -.912811   .6215921    -1.47   0.142    -2.132259    .3066366
         17  |  -.8743585   .5914991    -1.48   0.140    -2.034769    .2860523
         18  |  -.8364088   .5622412    -1.49   0.137    -1.939421    .2666035
         19  |   -.798962   .5338245    -1.50   0.135    -1.846226    .2483019
         20  |  -.7620179   .5062554    -1.51   0.133    -1.755196    .2311607
         21  |  -.7255767   .4795417    -1.51   0.131    -1.666348    .2151945
         22  |  -.6896382   .4536916    -1.52   0.129    -1.579696    .2004201
         23  |  -.6542025    .428715    -1.53   0.127    -1.495261    .1868562
         24  |  -.6192697   .4046225    -1.53   0.126    -1.413063     .174524
         25  |  -.5848396   .3814262    -1.53   0.125    -1.333127    .1634475
         26  |  -.5509123   .3591401    -1.53   0.125    -1.255478    .1536534
         27  |  -.5174878   .3377793    -1.53   0.126    -1.180148     .145172
         28  |  -.4845662   .3173612    -1.53   0.127     -1.10717    .1380373
         29  |  -.4521473   .2979051    -1.52   0.129    -1.036582    .1322869
         30  |  -.4202312   .2794321    -1.50   0.133    -.9684248    .1279625
         31  |  -.3888179   .2619656    -1.48   0.138    -.9027456    .1251097
         32  |  -.3579075   .2455306    -1.46   0.145    -.8395928    .1237778
         33  |  -.3274997   .2301536    -1.42   0.155    -.7790181    .1240186
         34  |  -.2975949   .2158615    -1.38   0.168    -.7210749    .1258851
         35  |  -.2681928    .202681    -1.32   0.186    -.6658152    .1294295
         36  |  -.2392935   .1906362    -1.26   0.210    -.6132861    .1346991
         37  |   -.210897   .1797466    -1.17   0.241    -.5635263    .1417322
         38  |  -.1830034   .1700241    -1.08   0.282     -.516559    .1505522
         39  |  -.1556125   .1614696    -0.96   0.335    -.4723858    .1611607
         40  |  -.1287244   .1540692    -0.84   0.404    -.4309795    .1735307
         41  |  -.1023391   .1477915    -0.69   0.489    -.3922785    .1876002
         42  |  -.0764567    .142585    -0.54   0.592    -.3561818    .2032685
         43  |  -.0510769   .1383779    -0.37   0.712    -.3225486    .2203948
         44  |     -.0262   .1350798    -0.19   0.846    -.2912013    .2388012
         45  |   -.001826   .1325846    -0.01   0.989    -.2619321    .2582802
         46  |   .0220454    .130776     0.17   0.866    -.2345126    .2786034
         47  |   .0454138   .1295329     0.35   0.726    -.2087055    .2995331
         48  |   .0682795   .1287349     0.53   0.596    -.1842744    .3208334
         49  |   .0906425   .1282674     0.71   0.480    -.1609941    .3422791
         50  |   .1125025   .1280245     0.88   0.380    -.1386577    .3636627
         51  |   .1338598   .1279125     1.05   0.296    -.1170807    .3848003
         52  |   .1547143   .1278507     1.21   0.226    -.0961049    .4055335
         53  |    .175066   .1277722     1.37   0.171    -.0755991    .4257311
         54  |    .194915    .127624     1.53   0.127    -.0554595    .4452895
         55  |   .2142611   .1273673     1.68   0.093    -.0356098     .464132
         56  |   .2331044   .1269768     1.84   0.067    -.0160004    .4822091
         57  |   .2514449   .1264408     1.99   0.047     .0033916    .4994982
         58  |   .2692826   .1257616     2.14   0.032     .0225619    .5160033
         59  |   .2866175   .1249551     2.29   0.022     .0414789     .531756
         60  |   .3034496   .1240522     2.45   0.015     .0600824    .5468169
         61  |   .3197789   .1230989     2.60   0.009     .0782818    .5612761
         62  |   .3356054   .1221575     2.75   0.006     .0959552    .5752557
         63  |   .3509291    .121307     2.89   0.004     .1129475    .5889108
         64  |   .3657501   .1206435     3.03   0.002       .12907    .6024301
         65  |   .3800682   .1202798     3.16   0.002     .1441017    .6160346
         66  |   .3938835   .1203431     3.27   0.001     .1577929    .6299742
         67  |   .4071961   .1209716     3.37   0.001     .1698723    .6445198
         68  |   .4200058    .122309     3.43   0.001     .1800583    .6599533
         69  |   .4323127   .1244965     3.47   0.001     .1880739    .6765515
         70  |   .4441168   .1276644     3.48   0.001      .193663    .6945705
         71  |   .4554181   .1319245     3.45   0.001     .1966068    .7142294
         72  |   .4662167   .1373635     3.39   0.001     .1967352    .7356981
         73  |   .4765124   .1440396     3.31   0.001     .1939335    .7590913
         74  |   .4863053    .151984     3.20   0.001     .1881411    .7844695
         75  |   .4955954    .161203     3.07   0.002     .1793452    .8118457
         76  |   .5043828   .1716843     2.94   0.003     .1675703    .8411952
         77  |   .5126673   .1834015     2.80   0.005     .1528677    .8724669
         78  |    .520449   .1963206     2.65   0.008     .1353047    .9055934
         79  |    .527728   .2104032     2.51   0.012     .1149562    .9404998
         80  |   .5345041   .2256102     2.37   0.018      .091899    .9771093
         81  |   .5407775   .2419038     2.24   0.026     .0662073    1.015348
         82  |    .546548   .2592487     2.11   0.035     .0379504    1.055146
         83  |   .5518157   .2776127     1.99   0.047     .0071914     1.09644
         84  |   .5565807    .296967     1.87   0.061    -.0260132    1.139175
         85  |   .5608428   .3172862     1.77   0.077    -.0616134    1.183299
         86  |   .5646022   .3385475     1.67   0.096    -.0995648    1.228769
         87  |   .5678587   .3607314     1.57   0.116    -.1398289    1.275546
         88  |   .5706125   .3838208     1.49   0.137    -.1823722    1.323597
         89  |   .5728634   .4078007     1.40   0.160    -.2271655    1.372892
         90  |   .5746116   .4326583     1.33   0.184    -.2741831    1.423406
         91  |   .5758569   .4583822     1.26   0.209    -.3234033    1.475117
         92  |   .5765995   .4849624     1.19   0.235    -.3748061    1.528005
         93  |   .5768392   .5123903     1.13   0.260    -.4283749    1.582053
         94  |   .5765762   .5406585     1.07   0.286    -.4840949    1.637247
         95  |   .5758103   .5697604     1.01   0.312    -.5419532    1.693574
         96  |   .5745417   .5996901     0.96   0.338    -.6019384    1.751022
         97  |   .5727702   .6304428     0.91   0.364    -.6640409    1.809581
         98  |    .570496   .6620137     0.86   0.389    -.7282512    1.869243
         99  |    .567719   .6943988     0.82   0.414    -.7945617        1.93
------------------------------------------------------------------------------

. 
. 
end of do-file

. 
. *       TABLE E1. Persuasive Effect by Treatment in Landry et al. (2006)
. *do "$Persuasion/scripts/tableE1.do"
. *       TABLE E2. Persuasive Effect by Treatment in DLM
. *do "$Persuasion/scripts/tableE2.do"
. 
. *       TABLE H1. Persuasion Rates: NTV Effects Using a Binary Instrument
. *do "$Persuasion/scripts/tableH1.do"
. 
. 
. 
. 
. * End log
. di "End date and time: $S_DATE $S_TIME"
End date and time: 11 Nov 2022 08:09:19

. log close
      name:  <unnamed>
       log:  /Users/sokbaelee/Dropbox/Persuasion/JPE/Replication Files/scripts/logs/2022.11.11-08.09.16.log.txt
  log type:  text
 closed on:  11 Nov 2022, 08:09:19
-------------------------------------------------------------------------------------------------------------------------------------------
